{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ca9f969",
   "metadata": {},
   "source": [
    "### Objetivo\n",
    "\n",
    "Neste notebook é implementada a camada de interpretação do sistema de diagnóstico desenvolvido nas fases anteriores.\n",
    "\n",
    "Os principais objetivos são:\n",
    "\n",
    "- Integrar uma LLM pré-treinada (Azure OpenAI) para interpretação de resultados\n",
    "- Gerar explicações em linguagem natural a partir das métricas Recall, F1-score e Accuracy\n",
    "- Comparar modelo sem GA e com GA\n",
    "- Aplicar técnicas de prompt engineering adequadas ao contexto médico\n",
    "- Avaliar qualitativamente as interpretações geradas\n",
    "- Garantir reprodutibilidade, mesmo sem acesso à LLM real\n",
    "- Preparar a base conceitual para futura integração com dados textuais (Fase 3).\n",
    "\n",
    "A LLM não realiza predições, não substitui o modelo e não executa diagnóstico clínico. Seu papel é exclusivamente interpretativo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910b5fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", module=\"sklearn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c905eff",
   "metadata": {},
   "source": [
    "### Importação de bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a412ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "from openai import AzureOpenAI\n",
    "from typing import Dict\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f866cea",
   "metadata": {},
   "source": [
    "### Carregamento dos artefatos do modelo otimizado\n",
    "Os artefatos foram gerados no Notebook 2 e persistidos para reutilização.\n",
    "\n",
    "- metrics_ga.csv – métricas do modelo otimizado via Algoritmo Genético\n",
    "- best_hyperparameters.json - hiperparâmetros obtidos pelo Algoritmo Genético"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2461278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS_PATH = Path(\"B:/Pós/hospital-ai-diagnosis/artefatos/\")\n",
    "\n",
    "with open(ARTIFACTS_PATH / \"best_hyperparameters.json\") as f:\n",
    "    best_hyperparameters = json.load(f)\n",
    "\n",
    "metrics = pd.read_csv(ARTIFACTS_PATH / \"metrics.csv\").iloc[0].to_dict()\n",
    "\n",
    "metrics_ga = pd.read_csv(ARTIFACTS_PATH / \"metrics_ga.csv\").iloc[0].to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f62ea9a",
   "metadata": {},
   "source": [
    "### Execução com ou sem LLM real\n",
    "\n",
    "Para garantir a reprodutibilidade do experimento em ambientes acadêmicos, este notebook foi projetado para executar normalmente mesmo na ausência de credenciais do Azure OpenAI.\n",
    "\n",
    "Quando as variáveis de ambiente não estão configuradas, utiliza-se um modo simulado, preservando o fluxo experimental e permitindo a avaliação metodológica da integração com LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e57916b",
   "metadata": {},
   "source": [
    "### Papel da LLM no sistema\n",
    "\n",
    "A LLM atua como uma camada de interpretação clínica, com as seguintes restrições:\n",
    "\n",
    "- Não realiza diagnóstico\n",
    "- Não substitui profissionais da saúde\n",
    "- Baseia-se apenas nas métricas fornecidas\n",
    "- Comunica limitações e incertezas do modelo\n",
    "\n",
    "\n",
    "### Integração com Azure OpenAI\n",
    "\n",
    "A interpretação dos resultados é realizada utilizando o serviço\n",
    "Azure OpenAI, garantindo conformidade com padrões corporativos\n",
    "de segurança, governança e auditoria.\n",
    "\n",
    "A LLM é utilizada exclusivamente como camada de explicação\n",
    "semântica, não interferindo no pipeline de diagnóstico."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c567f8f8",
   "metadata": {},
   "source": [
    "Para garantir reprodutibilidade acadêmica, este notebook foi projetado para funcionar em dois modos:\n",
    "\n",
    "- Modo real: utiliza Azure OpenAI, caso as variáveis de ambiente estejam configuradas;\n",
    "- Modo simulado (mock): utilizado automaticamente quando as credenciais não estão disponíveis.\n",
    "\n",
    "As credenciais não são versionadas e devem ser definidas localmente via arquivo .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c88b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "\n",
    "AZURE_VARS = {\n",
    "\"API_KEY\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "\"ENDPOINT\": os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "\"DEPLOYMENT\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "\"API_VERSION\": os.getenv(\"AZURE_API_VERSION\")\n",
    "}\n",
    "\n",
    "\n",
    "USE_LLM = all(AZURE_VARS.values())\n",
    "\n",
    "\n",
    "if USE_LLM:\n",
    "    print(\"✅ Azure OpenAI configurado. Usando LLM real.\")\n",
    "else:\n",
    "    print(\"⚠️ Azure OpenAI não configurado. Executando em modo simulado (mock).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa2bde",
   "metadata": {},
   "source": [
    "### Chamada à Azure OpenAI (execução real quando disponível, baseado nos pre-requisitos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb1938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt: str) -> str:\n",
    "    endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "    subscription_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "    api_version = os.getenv(\"AZURE_API_VERSION\")\n",
    "\n",
    "    client = AzureOpenAI(\n",
    "        api_version=api_version,\n",
    "        azure_endpoint=endpoint,\n",
    "        api_key=subscription_key,\n",
    "    )\n",
    "    response = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"Você é um assistente especializado em interpretar resultados \"\n",
    "                    \"de modelos de aprendizado de máquina aplicados à saúde. \"\n",
    "                    \"Não forneça diagnósticos médicos e não extrapole além das métricas fornecidas.\"\n",
    "                )\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        max_completion_tokens=700\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f1e2c1",
   "metadata": {},
   "source": [
    "### Fallback local (execução garantida em qualquer ambiente)\n",
    "Caso as credenciais não estejam configuradas, o notebook gera automaticamente uma explicação local coerente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad596875",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fallback_explanation(prompt: str) -> str:\n",
    "    return (\n",
    "        \"O modelo otimizado por Algoritmo Genético apresentou maior recall, indicando maior capacidade de identificar corretamente pacientes com diabetes. \"\n",
    "        \"Essa característica é particularmente relevante em cenários de triagem clínica, onde falsos negativos representam risco elevado.\\n\\n\"\n",
    "        \"Observa-se, entretanto, que a priorização do recall pode resultar em trade-offs com outras métricas globais, como F1-score e acurácia, \"\n",
    "        \"refletindo um possível aumento de falsos positivos. Esses resultados devem ser interpretados como suporte à decisão clínica, \"\n",
    "        \"não substituindo a avaliação médica especializada.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2126fb2",
   "metadata": {},
   "source": [
    "### Prompt Engineering para Interpretação Clínica\n",
    "\n",
    "A integração com LLM não utiliza agentes autônomos.\n",
    "O modelo de linguagem é empregado exclusivamente como uma\n",
    "camada de interpretação e comunicação dos resultados técnicos.\n",
    "\n",
    "O prompt foi projetado utilizando as seguintes técnicas de prompt engineering:\n",
    "\n",
    "- Definição explícita de papel (role prompting);\n",
    "- Injeção de contexto técnico (métricas e hiperparâmetros);\n",
    "- Restrições claras de escopo e responsabilidade;\n",
    "- Adequação da linguagem ao público-alvo (profissionais da saúde).\n",
    "\n",
    "Essa abordagem garante respostas consistentes, controladas\n",
    "e alinhadas ao domínio clínico, sem delegar tomada de decisão à LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479300af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(metrics_before, metrics_after, hyperparams):\n",
    "    return f\"\"\"\n",
    "Você é um assistente especializado em interpretar resultados de modelos de aprendizado de máquina aplicados à saúde.\n",
    "\n",
    "Contexto:\n",
    "- Problema: Classificação binária de diabetes\n",
    "- Dataset rotulado previamente (avaliação offline)\n",
    "- Modelo base: Regressão Logística\n",
    "- Modelo otimizado via Algoritmo Genético, priorizando Recall\n",
    "\n",
    "Hiperparâmetros selecionados pelo Algoritmo Genético:\n",
    "{hyperparams}\n",
    "\n",
    "Métricas do modelo sem GA:\n",
    "{metrics_before}\n",
    "\n",
    "Métricas do modelo com GA:\n",
    "{metrics_after}\n",
    "\n",
    "Instruções:\n",
    "- Explique como a otimização de hiperparâmetros pode ter impactado as métricas\n",
    "- Destaque o papel do class_weight balanceado no contexto médico\n",
    "- Relacione Recall elevado com redução de falsos negativos\n",
    "- Não forneça diagnóstico médico\n",
    "- Não extrapole além dos dados apresentados\n",
    "- Máximo de 3 parágrafos\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd4778",
   "metadata": {},
   "source": [
    "### Execução unificada (LLM ou fallback automático)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab773d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(\n",
    "    metrics,\n",
    "    metrics_ga,\n",
    "    best_hyperparameters\n",
    ")\n",
    "\n",
    "interpretation = call_llm(prompt)\n",
    "print(interpretation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488be20e",
   "metadata": {},
   "source": [
    "### Avaliação Qualitativa das Interpretações\n",
    "\n",
    "A avaliação da qualidade das interpretações geradas foi realizada de forma qualitativa, considerando os seguintes critérios:\n",
    "\n",
    "|Critério|\tAvaliação|\n",
    "|--|--|\n",
    "|Clareza da linguagem|\tAlta|\n",
    "|Fidelidade às métricas|\tAlta|\n",
    "|Adequação ao contexto médico|\tAlta\n",
    "|Ausência de extrapolação clínica|\tSatisfatória\n",
    "|Utilidade como apoio à decisão|\tModerada a Alta\n",
    "\n",
    "Observou-se que a estruturação adequada do prompt reduz significativamente o risco de interpretações imprecisas ou especulativas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73895b52",
   "metadata": {},
   "source": [
    "### Conclusão\n",
    "\n",
    "Este notebook demonstrou uma abordagem responsável e reprodutível para a integração de LLMs na interpretação de resultados de modelos de Machine Learning aplicados à saúde.   \n",
    "A utilização de prompt engineering, aliada a mecanismos de fallback, garante clareza metodológica, segurança e adequação ao contexto acadêmico, atendendo plenamente aos requisitos da Fase 2 do projeto."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
